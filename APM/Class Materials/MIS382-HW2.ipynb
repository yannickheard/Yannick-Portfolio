{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\">Due: Wed, October 4th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Bias-variance Trandeoff (2pts)\n",
    "How does the choice of K in the K-nearest neighbor classifier reflect a bias-variance tradeoff?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Data Exploration and Regression Analysis (4+3+4+4=15pts)\n",
    "\n",
    "Consider the dataset provided (russett_full_v2.csv) about agricultural inequality, industrial development and political instability in different countries. More information about it can be found [here](https://www.rdocumentation.org/packages/plspm/versions/0.4.9/topics/russett) though the data itself is slightly different than that referenced in the link.\n",
    "\n",
    "a) (4 points) Generate box-plots of the \"rent\" (% of farmers that rent all their land), \"inst\" (measure of political stability in the executive branch), \"ecks\" (number of violent internal war incidents ) and \"demo_score\" ( derived measure of the level of a country's democracy from 1945 to 61 ) and identify the cutoff values for outliers. \n",
    "\n",
    "Generate 3 scatterplots of \"rent\" against \"demo_score\", \"inst\" against \"demo_score\" and \"ecks\" against \"demo_score\" with the identified outliers colored differently than non-outliers in each; comment on how inclusion of the outliers would affect a predictive model for the \"demo_score\" response.  \n",
    "\n",
    "b) (3 points) Let us try to fit an MLR, using ordinary least squares, to this dataset with \"demo_score\" as the dependent variable using only the predictors 'rent','inst', and 'ecks' . \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state=22)   \n",
    "\n",
    "Report the RMSE obtained on both X_train and X_test. How much does this increase when you score your model on X_test?\n",
    "\n",
    "c) (4 points ) Try to predict ”demo_score” using a robust regression using Huber loss.  You can use the [sklearn package](  http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html).  Set regularization parameter alpha to 0.0 and all other parameters as default.\n",
    "Report RMSE obtained on both X_train and X_test.\n",
    "\n",
    "d) (4 points ) Compare and comment on the model fits obtained in (b) and (c) and plot the residual plots using all data for each model. How do outliers affect the relative performance of ordinary least squares regression, and robust regression with Huber loss in general? \n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Sampling (4+4=8pts)\n",
    "\n",
    "1. NBC has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a “black and white age”). NBC wants to estimate what fraction p of its audience like the show by “randomly” calling n viewers and tallying their responses so as to estimate the true value of p to a fractional accuracy of within ±ε%, with a confidence of (1 − α) × 100%. For α = 0.10, ε = 0.03 (i.e. your answer will be $\\hat{p}$ ± 0.03), what is the minimum value of n needed if true value (i) p = 0.5, (ii) p=0.25? \n",
    "\n",
    "2. Suppose for a certain value of p and choice of ε, you calculate that you will need 1000 samples for α = 0.02. You now decide to obtain a more accurate answer by either (i) reducing α to 0.01, keeping the same ε or by (ii) reducing ε by a factor of 2 from the original value, and increasing α to 0.05. In each case how many samples would you need now?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Principal Component Analysis (10 points)\n",
    "\n",
    "Import the diabetes dataset as in Q1 and add the interaction variables.\n",
    "You should have 65 variables and one target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to center and scale each feature as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdScaler = StandardScaler()\n",
    "X = stdScaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform PCA using the sklearn [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package.  Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X matrix).  Refer to Figure 10.4 of JW for an example.  You may use the output attribute *explained variance ratio*. (3pts)\n",
    "\n",
    "(b) How many principal components (N1, N2, N3) are required to explain cumulative variance of 30%, 60%, and 90%, respectively? (3pts)\n",
    "\n",
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. (This is called Principal Components Regression) Use entire dataset, e.g. 442 rows. Evaluate the models using mean squared error (MSE). (4pts)\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 -  Feature Selection ( 5 points )\n",
    "\n",
    "Explain what you understand by the two wrapper methods for feature selection (forward and backward selection) (no more than 1 paragraph). \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
