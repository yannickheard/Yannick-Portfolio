{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "%pylab inline\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "from pandas import Series, DataFrame\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A (Basic Text Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 5 parts of speech in this corpus of job descriptions? How frequently do they appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data\n",
    "- First the csv file was read and named `jobs`. \n",
    "- A sample of 10,000 rows was pulled and a seed set for reproducibility. \n",
    "- The first 5 rows are printed for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>SalaryTarget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179687</th>\n",
       "      <td>71565145</td>\n",
       "      <td>SAS Administrator  London  ****K</td>\n",
       "      <td>SAS Administrator  London  ****k My client, ar...</td>\n",
       "      <td>City London South East</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Annapurna</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>45000 - 55000 per annum + benefits</td>\n",
       "      <td>50000</td>\n",
       "      <td>cwjobs.co.uk</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18497</th>\n",
       "      <td>67016578</td>\n",
       "      <td>Retail Director</td>\n",
       "      <td>Job Title: Retail Director Location: Midlands ...</td>\n",
       "      <td>South East</td>\n",
       "      <td>South East London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COREcruitment International</td>\n",
       "      <td>Hospitality &amp; Catering Jobs</td>\n",
       "      <td>85000-110000 Per Annum 85,000 to 110,000 plus ...</td>\n",
       "      <td>97500</td>\n",
       "      <td>caterer.com</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86382</th>\n",
       "      <td>69090255</td>\n",
       "      <td>Private Client Solcitor</td>\n",
       "      <td>Private Client Solicitor  2 years PQE  East Gr...</td>\n",
       "      <td>Tonbridge</td>\n",
       "      <td>Tonbridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Select Appointments</td>\n",
       "      <td>Legal Jobs</td>\n",
       "      <td>40K</td>\n",
       "      <td>40000</td>\n",
       "      <td>legalprospects.com</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12805</th>\n",
       "      <td>66576936</td>\n",
       "      <td>Territory Development Manager</td>\n",
       "      <td>****mh**** Devon &amp; Cornwall Excellent opportun...</td>\n",
       "      <td>Devon, South West</td>\n",
       "      <td>Devon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Retail Jobs</td>\n",
       "      <td>22000 per annum + Car Allowance, Mobile, Laptop</td>\n",
       "      <td>22000</td>\n",
       "      <td>jobs.telegraph.co.uk</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203497</th>\n",
       "      <td>71941287</td>\n",
       "      <td>White Goods/Domestic Appliance Engineer</td>\n",
       "      <td>Position: White Goods/Domestic Appliance Engin...</td>\n",
       "      <td>Shrewsbury Shropshire West Midlands</td>\n",
       "      <td>Shrewsbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>24000</td>\n",
       "      <td>24000</td>\n",
       "      <td>technojobs.co.uk</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                    Title  \\\n",
       "179687  71565145         SAS Administrator  London  ****K   \n",
       "18497   67016578                          Retail Director   \n",
       "86382   69090255                  Private Client Solcitor   \n",
       "12805   66576936            Territory Development Manager   \n",
       "203497  71941287  White Goods/Domestic Appliance Engineer   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "179687  SAS Administrator  London  ****k My client, ar...   \n",
       "18497   Job Title: Retail Director Location: Midlands ...   \n",
       "86382   Private Client Solicitor  2 years PQE  East Gr...   \n",
       "12805   ****mh**** Devon & Cornwall Excellent opportun...   \n",
       "203497  Position: White Goods/Domestic Appliance Engin...   \n",
       "\n",
       "                                LocationRaw LocationNormalized ContractType  \\\n",
       "179687               City London South East             London          NaN   \n",
       "18497                            South East  South East London          NaN   \n",
       "86382                             Tonbridge          Tonbridge          NaN   \n",
       "12805                     Devon, South West              Devon          NaN   \n",
       "203497  Shrewsbury Shropshire West Midlands         Shrewsbury          NaN   \n",
       "\n",
       "       ContractTime                      Company                     Category  \\\n",
       "179687    permanent                    Annapurna                      IT Jobs   \n",
       "18497           NaN  COREcruitment International  Hospitality & Catering Jobs   \n",
       "86382     permanent         Select Appointments                    Legal Jobs   \n",
       "12805     permanent                          NaN                  Retail Jobs   \n",
       "203497    permanent                          NaN                      IT Jobs   \n",
       "\n",
       "                                                SalaryRaw  SalaryNormalized  \\\n",
       "179687                 45000 - 55000 per annum + benefits             50000   \n",
       "18497   85000-110000 Per Annum 85,000 to 110,000 plus ...             97500   \n",
       "86382                                                 40K             40000   \n",
       "12805     22000 per annum + Car Allowance, Mobile, Laptop             22000   \n",
       "203497                                              24000             24000   \n",
       "\n",
       "                  SourceName SalaryTarget  \n",
       "179687          cwjobs.co.uk         High  \n",
       "18497            caterer.com         High  \n",
       "86382     legalprospects.com          Low  \n",
       "12805   jobs.telegraph.co.uk          Low  \n",
       "203497      technojobs.co.uk          Low  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = pd.read_csv('Train_rev1.csv', encoding = 'latin1')\n",
    "corpora = jobs.sample(n=10000, random_state = 1)\n",
    "corpora[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def token_tag_count(string):\n",
    "    #1. tokenize the string\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    #2. tag the tokens\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    #3. find the frequencies\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_tokens)\n",
    "    x = tag_fd.most_common()\n",
    "    #append to the main list\n",
    "    list_of_pos_counts.append(x)\n",
    "    return\n",
    "\n",
    "#initialize the main list\n",
    "list_of_pos_counts = []\n",
    "#apply the function to each job description\n",
    "corpora['FullDescription'].apply(token_tag_count)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the sums of parts of speech\n",
    "\n",
    "- A new data frame was created `df_all` with two columns, `POS` identifies the Part of Speech and `POS_Counts` identifies the frequency for which the part of speech appears in the original 10,000 job descriptions.\n",
    "- Then a function was created `AddRowtoDF` that adds the tokenized words (as a list of tuples) into the `df_all` data frame. - Then a loop was used to apply the `AddRowtoDF` function to each of the 10,000 rows.\n",
    "- To find the sums, `groupby` function was utilized to group the parts of speech with their respective counts, then call the `sum` aggregate function to gather the counts.\n",
    "- Counts have been sorted below in descending order to show the top five parts of speech and the frequency for which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_Counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>453861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>338725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>257078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>214486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>199035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     POS_Counts\n",
       "POS            \n",
       "NN       453861\n",
       "NNP      338725\n",
       "IN       257078\n",
       "JJ       214486\n",
       "DT       199035"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize an empty dataframe\n",
    "df_all = pd.DataFrame(columns=['POS','POS_Counts'])\n",
    "\n",
    "def AddRowToDF(listoftuples):\n",
    "    global df_all\n",
    "    #create a small dataframe\n",
    "    df = pd.DataFrame(listoftuples)\n",
    "    #name the columns\n",
    "    df.columns = ['POS','POS_Counts']\n",
    "    #append the small dataframe to the master dataframe\n",
    "    df_all = df_all.append(df)\n",
    "    \n",
    "for i in list_of_pos_counts:\n",
    "    AddRowToDF(i)\n",
    "\n",
    "Grp_agg = df_all.groupby(['POS'])[['POS_Counts']].agg('sum')\n",
    "Grp_agg.sort_values('POS_Counts', ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The below keys identify what each part of speech abbreviation represents.\n",
    "\n",
    "NN: noun, common, singular or mass<br>\n",
    "NNP: noun, proper, singular<br>\n",
    "IN: preposition or conjunction, subordinating<br>\n",
    "JJ: adjective or numeral, ordinal<br>\n",
    "DT: determiner<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A2 - Does this corpus support Zipf's law? Plot the most common 100 words against the theoretical prediction of the law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Log Frequency</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Log Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>106743</td>\n",
       "      <td>5.028339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>83804</td>\n",
       "      <td>4.923265</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>80839</td>\n",
       "      <td>4.907621</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>60460</td>\n",
       "      <td>4.781468</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>57341</td>\n",
       "      <td>4.758465</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word  Frequency  Log Frequency  Rank  Log Rank\n",
       "0  and     106743       5.028339   1.0  0.000000\n",
       "1  the      83804       4.923265   2.0  0.301030\n",
       "2   to      80839       4.907621   3.0  0.477121\n",
       "3    a      60460       4.781468   4.0  0.602060\n",
       "4   of      57341       4.758465   5.0  0.698970"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_words = pd.Series(\" \".join(corpora['FullDescription']).lower().split()).value_counts()[:100]\n",
    "top_100_df = pd.DataFrame(top_100_words)\n",
    "top_100_df.reset_index(inplace=True)\n",
    "top_100_df.columns = ['Word', 'Frequency']\n",
    "type(top_100_df['Frequency'][0])\n",
    "\n",
    "top_100_df['Log Frequency'] = top_100_df['Frequency'].map(math.log10)\n",
    "top_100_df['Rank'] = top_100_df['Frequency'].rank(ascending=False)\n",
    "top_100_df['Log Rank'] = top_100_df['Rank'].map(math.log10)\n",
    "top_100_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOOeci8UThnPOuVj+H3yXL63xQ7NyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a4fe2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(top_100_df['Log Rank'], top_100_df['Log Frequency'], marker = 'o', linestyle = 'None')\n",
    "xlabel('Log Rank')\n",
    "ylabel('Log Frequency')\n",
    "title('Log Frequency vs. Log Rank')\n",
    "\n",
    "def graph(formula, x_range):  \n",
    "    x = np.array(x_range)  \n",
    "    y = eval(formula)\n",
    "    plt.plot(x, y)  \n",
    "    plt.show()\n",
    "graph('-x+5.03', range(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The corpus above does indeed follow Zipf's Law. The orange line above is a plot of the inverse relationship between log frequency and log rank (as described by Zipf's Law). The trend of the 100 most common words (as indicated by the blue scatter points) also follow a similar negative trend, although the trend is not perfectly linear due to the frequency of words varying in different magnitudes in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A3. If the stopwords are removed and the corpus lemmatized, what are the 10 most common words? What are their frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words: \n",
      "experience    17514\n",
      "role          12561\n",
      "team          12387\n",
      "work          12111\n",
      "client        11344\n",
      "business      10839\n",
      "job           10174\n",
      "skill          9937\n",
      "service        9924\n",
      "working        8881\n",
      "Name: Stem Words, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def removewords(string):\n",
    "    global df_of_filtered_words\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string) \n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    filtered_words = [word.lower() for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "    df = pd.DataFrame(filtered_words)\n",
    "    df.columns = ['words']\n",
    "    df_of_filtered_words = df_of_filtered_words.append(df)\n",
    "    #df_of_filtered_words\n",
    "    return\n",
    "\n",
    "df_of_filtered_words = pd.DataFrame(columns=['words'])\n",
    "corpora['FullDescription'].apply(removewords)\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "df_of_filtered_words['Stem Words'] = df_of_filtered_words['words'].map(lemma.lemmatize)\n",
    "print \"10 most common words: \"\n",
    "print df_of_filtered_words['Stem Words'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part B (Predicting Salary from Job Description)\n",
    "\n",
    "In this section, you will create classification models to predict high (75th percentile and above) or low (below 75th percentile) salary from the text contained in the job descriptions. Use the Naïve Bayes classifier to classify job descriptions into high and low salary categories. For all models below, show the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B1. Create a classification model with all words and the bag-of-words approach. How accurate is the model (show the confusion matrix)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#set up the target column with high/low\n",
    "high_salary = corpora['SalaryNormalized'].quantile(0.75)\n",
    "high_salary\n",
    "\n",
    "def salary_class(salary):\n",
    "    if salary >= high_salary:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "    \n",
    "corpora['SalaryTarget'] = corpora['SalaryNormalized'].map(salary_class)\n",
    "corpora['SalaryTarget'][:10]\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5000)\n",
      "0.7861\n",
      "[[2034  476]\n",
      " [1663 5827]]\n"
     ]
    }
   ],
   "source": [
    "def job_to_words(job_description):\n",
    "    # Function to convert a raw job description to a string of words\n",
    "    # The input is a single string (a raw ob description), and \n",
    "    # the output is a single string (a preprocessed ob description)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", job_description) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                              \n",
    "    #\n",
    "    # 3. Join the words back into one string separated by space, \n",
    "    onestring = (\" \".join(words)) \n",
    "    #\n",
    "    # 4. append the onestring to the main list\n",
    "    clean_job_descriptions.append(onestring)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_job_descriptions = []\n",
    "\n",
    "# Loop over each job description; create an index i that goes from 0 to the length\n",
    "corpora['FullDescription'].map(job_to_words)\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_job_descriptions)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data_features, corpora['SalaryTarget'])\n",
    "\n",
    "# make predictions\n",
    "expected = corpora['SalaryTarget']\n",
    "predicted = model.predict(train_data_features)\n",
    "# summarize the fit of the model\n",
    "print(metrics.accuracy_score(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above model, the classification accuracy is 78.61%. This will be used as the baseline moving forward, since no additional techniques were used to further pre-process the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Speculate before running the following analysis whether lemmatization would help improve the accuracy of classification. Now create a classification model after lemmatization. Did the classification accuracy increase relative to B1? Comment on your speculation versus the actual results you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speculation: \n",
    "\n",
    "- Since the analysis is being limited to 5000 features in the CountVectorizer, then lemmatizing words in theory will allow us to get more words than the original list. As an example, `experience` and `experiences` are now one feature identified from their stems. Therefore, I expect lemmatization to slightly improve the accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7886\n",
      "[[2035  475]\n",
      " [1639 5851]]\n"
     ]
    }
   ],
   "source": [
    "def job_to_words2(job_description):\n",
    "    # Function to convert a raw job description to a string of words\n",
    "    # The input is a single string (a raw ob description), and \n",
    "    # the output is a single string (a preprocessed ob description)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", job_description) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "    \n",
    "    # 3. Lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    meaningful_words = []\n",
    "    for w in words:\n",
    "        lemmaword = lemma.lemmatize(w)\n",
    "        meaningful_words.append(lemmaword)\n",
    " \n",
    "    # 4. Join the words back into one string separated by space, \n",
    "    onestring = (\" \".join(meaningful_words)) \n",
    "    #\n",
    "    # 5. append the onestring to the main list\n",
    "    clean_job_descriptions.append(onestring)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_job_descriptions = []\n",
    "\n",
    "# Loop over each job description; create an index i that goes from 0 to the length\n",
    "corpora['FullDescription'].map(job_to_words2)\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_job_descriptions)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data_features, corpora['SalaryTarget'])\n",
    "\n",
    "# make predictions\n",
    "expected = corpora['SalaryTarget']\n",
    "predicted = model.predict(train_data_features)\n",
    "# summarize the fit of the model\n",
    "print(metrics.accuracy_score(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared to Part B1, there is a slight improvement in classification accuracy (78.86% vs. 78.61%). This was in line with the original speculation, but not to the degree expected. This may have been due to the fact that the accuracy was already relatively high, and any additional processing would be small and incremental towards improving accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Now speculate whether stopwords removal from the original data would help increase the accuracy of the model. Take out the stopwords (but do not lemmatize), build a classification model and check the accuracy, and compare with that in B1 & B2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speculation:\n",
    "\n",
    "- Similar to the above scenario with lemmatization, there is an expectation of an improvement in the classification accuracy once stopword removal is done. This is because stop words are not indicative of salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7935\n",
      "[[2062  448]\n",
      " [1617 5873]]\n"
     ]
    }
   ],
   "source": [
    "def job_to_words3(job_description):\n",
    "    # Function to convert a raw job description to a string of words\n",
    "    # The input is a single string (a raw ob description), and \n",
    "    # the output is a single string (a preprocessed ob description)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", job_description) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    onestring = ( \" \".join( meaningful_words )) \n",
    "    #\n",
    "    # 7. append the onestring to the main list\n",
    "    clean_job_descriptions.append(onestring)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_job_descriptions = []\n",
    "\n",
    "# Loop over each job description; create an index i that goes from 0 to the length\n",
    "corpora['FullDescription'].map(job_to_words)\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = 'english',   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_job_descriptions)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data_features, corpora['SalaryTarget'])\n",
    "\n",
    "# make predictions\n",
    "expected = corpora['SalaryTarget']\n",
    "predicted = model.predict(train_data_features)\n",
    "# summarize the fit of the model\n",
    "print(metrics.accuracy_score(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After removing stop words, the classification accuracy was 79.35%. This was the highest accuracy so far, and is consistent with the speculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the top 10 words (excluding stopwords) that are most indicative of (i) high salary, and (ii) low salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High salary indicators:\n",
      "experience   -4.199473\n",
      "business     -4.519504\n",
      "role         -4.733370\n",
      "team         -4.790975\n",
      "management   -4.855313\n",
      "work         -4.889473\n",
      "skills       -4.942617\n",
      "client       -5.028459\n",
      "working      -5.104333\n",
      "manager      -5.108927\n",
      "dtype: float64\n",
      "\n",
      "Low salary indicators:\n",
      "experience   -4.404163\n",
      "work         -4.702867\n",
      "role         -4.737669\n",
      "team         -4.799539\n",
      "skills       -4.929025\n",
      "job          -4.949979\n",
      "sales        -4.957353\n",
      "working      -4.986955\n",
      "business     -5.023334\n",
      "client       -5.199505\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Log of the likelihoods for class 0 (High salary) and class 1 (Low salary)\n",
    "\n",
    "# exp(model.class_log_prior_[0]) # checking to see which class is class 0\n",
    "\n",
    "high_important = model.feature_log_prob_[0]\n",
    "low_important = model.feature_log_prob_[1]\n",
    "\n",
    "high_importance_series = Series(high_important, index=vectorizer.get_feature_names())\n",
    "print \"High salary indicators:\\n\", high_importance_series.sort_values(ascending=False)[:10]\n",
    "\n",
    "low_importance_series = Series(low_important, index=vectorizer.get_feature_names())\n",
    "print \"\\nLow salary indicators:\\n\", low_importance_series.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4.  Use the job descriptions without lemmatiztion and stopword removal. Add parts-of-speech bigrams to the bag-of-words, and run a new classification model. Does the accuracy increase over the results in B1? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(string):\n",
    "    array_of_tokens = []\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.7865\n",
      "[[2017  493]\n",
      " [1642 5848]]\n"
     ]
    }
   ],
   "source": [
    "def job_to_words4(job_description):\n",
    "    # Function to convert a raw job description to a string of words\n",
    "    # The input is a single string (a raw ob description), and \n",
    "    # the output is a single string (a preprocessed ob description)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", job_description) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()  \n",
    "    #\n",
    "    # 3. Join the words back into one string separated by space, \n",
    "    onestring = (\" \".join(words)) \n",
    "    #\n",
    "    # 4. append the onestring to the main list\n",
    "    clean_job_descriptions.append(onestring)\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_job_descriptions = []\n",
    "\n",
    "# Loop over each job description; create an index i that goes from 0 to the length\n",
    "corpora['FullDescription'].map(job_to_words4)\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = custom_tokenizer,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_job_descriptions)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# print train_data_features.shape\n",
    "\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data_features, corpora['SalaryTarget'])\n",
    "\n",
    "# make predictions\n",
    "expected = corpora['SalaryTarget']\n",
    "predicted = model.predict(train_data_features)\n",
    "# summarize the fit of the model\n",
    "print(metrics.accuracy_score(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From this final model, the classification accuracy was 78.65%. Compared to the results of Part B1, there was a very slight improvement of 0.04% in accuracy. The only difference is that this model also correlates the words with their part of speech - so no additional features are added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
