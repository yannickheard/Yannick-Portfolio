---
title: "OM 380.17: Supply Chain Analytics - MSBA"
subtitle: "Solution to Assignment #3"
output: html_notebook

---

The sales data is provided in CSV format in the file **"Peanut Butter Chicago.csv"**. As this is an individual skill-building assignment (as opposed to an open-ended team assignment), and I would like to achieve some degree of convergence in your answers, I have also provided a preprocessing script for the assignment.  The data set corresponds to the total weekly sales of peanut butter for the supermarket chain, not the individual stores. As you can observe from the file, the data corresponds to a combination of multiple brands as well as the supermarket private label (generic) in sizes ranging from 0.75 to 1.5 lbs.   

The data includes the following information for each individual stock keeping unit (SKU) as identified by its UPC code on each week in the data file:

* VEND	Number identifying the product vendor (48001 corresponds to Unilever).
* UPC	The product’s universal product code (bar code)
* UNITS	Sales volume
* DOLLARS	Dollar sales revenue
*	VOL_EQ	Weight in pounds of a units sold
*	PPU	Price per unit ($/lb)
*	F	Factor specifying advertising in the store weekly flyer:  
    + F = “A”	Large size ad.
    + F = “B”	Medium size ad.
    + F = “C”	Small size ad.
*	D	Factor specifying In-Store Display
    + D = 0	No In-Store Display
    + D = 1	Minor In-Store Display
    + D = 2	Major In-Store Display

To simplify the modeling process (and the assignment) in the preprocessing script I lumped all products into just three aggregate products (sub-categories): “SK” includes all Skippy brand products, “OB” includes all other branded products and “PL” includes all private label products. For each of the three aggregate products I obtained total sales (volume), average sale prices, and volume-weighted averages of the advertising and display variables (F and D).  Please take a few minutes to examine the included pre-processing script **“Assignment 3 Pre-Processing.R”** 

 
Our goal is to embed a log-log demand model in an ARIMA model that accounts for the auto-correlations in the sales data.  As a first attempt we would like to include a model of the following form:

$$y=e^{βx} p_S^α p_B^γ p_P^{γ_o}$$

Where the model variables and parameters are defined as follows:

* $y$ :	Demand (sales volume)
* $p_S$ :	Average price per pound of “Skippy” products
* $p_B$ :	Average price per pound of “Other Branded” products
* $p_P$ :	Average price per pound of “Private Label” products
* $x$ :	Weighted averages of advertising and display variables for each product sub-category
* $β$ :	Vector of coefficients for advertising and display variables
* $α,γ,γ_o$:	Coefficients of average prices

We have a total of 104 weeks of data.  In this assignment we will use weeks 1 through 94 as a training set and weeks 95 through 104 as a testing set.


1. (5 pts) After pre-processing the data, notice that you have 18 predictive variables plus the sales vector.  Notice that the pre-processing step already computes the log of the average prices and sales volumes. Use The Lasso on the training set to obtain (a) a shrunk model and (b) the reduced set of predictive variables minimizing the cross-validated MSE over the training set. (Use set.seed(1) before 10-ford cross-validation).  Report the coefficients of the shrunk model.

```{r, message=FALSE, warning=FALSE}
library(fpp)
library(reshape)
library(dplyr)
library(glmnet)

# Data Pre-Processing 
#
PBS <- read.csv("Peanut Butter Chicago.csv")[,-1] %>% 
  mutate( F_LSA=ifelse(F=="A",1,0),   # Large Size Ad Dummy
          F_MSA=ifelse(F=="B",1,0),   # Medium Size Ad Dummy
          F_SSA=ifelse(F=="C",1,0),   # Small Size Ad Dummy
          D_MIN=ifelse(D==1,1,0),     # Minor In-Store Display Dummy
          D_MAJ=ifelse(D==2,1,0)) %>% # Major In-Store Display Dummy
  # Promotional variables are weighted by sales volume (oz)
  mutate(S_LB = UNITS * VOL_EQ,
         WF_LSA = F_LSA * UNITS * VOL_EQ,     # Large Size Ad Weighted
         WF_MSA = F_MSA * UNITS * VOL_EQ,     # Medium Size Ad Weighted
         WF_SSA = F_SSA * UNITS * VOL_EQ,     # Small Size Ad Weighted
         WD_MIN = D_MIN * UNITS * VOL_EQ,     # Minor In-Store Display Weighted
         WD_MAJ = D_MAJ * UNITS * VOL_EQ) %>% # Major In-Store Display Weighted
  mutate(VEND =ifelse(VEND == 48001,"SK",ifelse( VEND == 99998,"PL","OB"))) %>%
  select(-F, -D)

# Create aggregate variables by product-week
x.pw <- group_by(PBS, WEEK, VEND) %>% 
  summarise(S.DOLLARS = sum(DOLLARS),      # Total $ Sales 
            S.S_LB    = sum(S_LB),         # Total L. Sales
            S.WF_LSA  = sum(WF_LSA),       # Total Weighted Large Ad
            S.WF_MSA  = sum(WF_MSA),       # Total Weighted Medium Ad
            S.WF_SSA  = sum(WF_SSA),       # Total Weighted Small Ad
            S.WD_MIN  = sum(WD_MIN),       # Total Weighted Minor Store Disp
            S.WD_MAJ  = sum(WD_MAJ)) %>%   # Total Weighted Major Store Disp
  # Calculate weigted averages of Advertising and Promotion variables
  mutate(A.PPU = log(S.DOLLARS / S.S_LB),  # Avg. Price per unit (pound)
         S.WF_LSA  = S.WF_LSA / S.S_LB,    # Avg. Weighted Large Ad
         S.WF_MSA  = S.WF_MSA / S.S_LB,    # Avg. Weighted Medium Ad
         S.WF_SSA  = S.WF_SSA / S.S_LB,    # Avg. Weighted Small Ad
         S.WD_MIN  = S.WD_MIN / S.S_LB,    # Avg. Weighted Minor Store Disp
         S.WD_MAJ  = S.WD_MAJ / S.S_LB)    # Avg. Weighted Major Store Disp

#
xmat <- x.pw %>%
  mutate(LS  = log(S.S_LB)) %>% 
  select(-S.DOLLARS, -S.S_LB)
#
# Creeate separate columns for vars of each brand group
xmat <- data.frame(filter(xmat, VEND == "SK"),
                   filter(xmat, VEND == "OB"),
                   filter(xmat, VEND == "PL")) %>%
  select(-WEEK, -WEEK.1, -WEEK.2, 
         -VEND, -VEND.1, -VEND.2, 
         -LS.1, -LS.2) # After droping vars. you should have 19 vars left

#
xm <- model.matrix(LS ~., data=xmat)[,-1]
y <- xmat[,"LS"]
#
```

```{r, fig.height=4, fig.width=6}
xm.tr <- xm[1:94,]
y.tr <-  y[1:94]
xm.te <- xm[95:104,]
y.te <-  y[95:104]
#
set.seed(1)
m.L <- cv.glmnet(x=xm.tr, y=y.tr,  alpha=1)
plot(m.L)
L.min <- m.L$lambda.min
L.1se <- m.L$lambda.1se
coefficients(m.L, s=c(L.min,L.1se))
```


2. (5 pts) Use the training set to fit an unrestricted regression model (i.e., lm(…) ) on the reduced set of explanatory variables identified by The Lasso.  Report the coefficients of the full model and comment on the fit of the model and examine the auto-correlations of the residuals of this model. 

```{r}
md.SK <- lm(y ~ xm[,c(4,6)])
summary(md.SK)
tsdisplay(residuals(md.SK))
```

*The coefficients of the "Minor In-Store Display" variable, and of the "Price Elasticity" parameter are signifficant in the unconstrained (plain) regression, but the residuals show auto-correlation. Hence the model is not valid.*  


3. (5 pts) Fit an ARIMA model to explain the training set log-of-sales-volume data.  Report the diagnostic of your model’s residuals and comment on the model’s validity.  

```{r, fig.height=4, fig.width=6}
y.LS <- ts(y, start=1)
y.tr <- window(y.LS, end=94)
y.te <- window(y.LS, start=95)
plot(y.LS)
maa.SK = auto.arima(y.tr)
summary(maa.SK)
tsdiag(maa.SK)
```

*The "plain-vanilla" ARIMA (2,0,2) fitted by auto.arima appears to be valid (i.e., there is no reason to reject the hypothesis that the residuals are uncorrelated).*  


4. (5 pts) Use the model in Question 3 to prepare a 10 period ahead forecast and compare it (overly it) with the testing set log-of-sales data.  Comment on the usefulness of this model in terms of precision and confidence interval. 

```{r, fig.height=4, fig.width=6}
fc.SK <- forecast(maa.SK, h=10)
plot(fc.SK)
points(y.te, col="red", pch=19)
```

*Although in the previous question we determined that he model is valid, it is not very useful for prediction as the confidence interval exposes two shortcomings of tghis mode:*  
*(1) It is too wide; it ranges over all the previously observed demands, and*  
*(2) it is not granular enough; it fails to indicate in advance if the demand will increase or decrease.*

5. (5 pts) Use the auto.arima(…) function to fit a dynamic regression model to explain sales data (log) using only the predictive variables identified by The Lasso in Question 1.  Examine the model’s residuals and comment on its validity. 

```{r, fig.height=4, fig.width=6}
maa.SKX <- auto.arima(y.tr, xreg=xm.tr[,c(4,6)])
summary(maa.SKX)
tsdiag(maa.SKX)
```

*The validity of the model is marginal as we come very close to rejecting the residual independence hypothesis (Ljung-Box test) at lags of 9 and 10.*


6. (5 pts) Obtain a dynamic regression model that improves on the auto-arima model in Question 5 in terms of its information coefficients and residual diagnostics. Compare the coefficients of the explanatory variables in (a) The Lasso model, (b) The unrestricted model obtained in Question 2, and (c) The ones obtained in this question.  Then use the B notation (polynomial) to describe the model you obtained.  

```{r, fig.height=4, fig.width=6}
#
# After some experimentation, I settled on the model below
# your answer may differ on the order of the model
#
m.SKX <- Arima(y.tr, order=c(3,0,2), xreg=xm.tr[,c(4,6)])
tsdisplay(diff(arima.errors(m.SKX)))
tsdisplay(residuals(m.SKX))
summary(m.SKX)
tsdiag(m.SKX)
```

*Coefficient Comarison:*  

| **Coefficient** | **Lasso** (min) | **Plain Reg.**| **ARIMA** |
|:----------------|----------------:|--------------:|----------:|
| **S.WD_MIN**    | 0.479           | 0.522         | 0.359     |
| **A.PPU**       | -2.421          | -2.581        | -2.6548   |

*The estimate of the price elasticity of demand is closer between the Dinamic Regression and the plain Regression model, but the autocorrelation was causing an overestimation of the effgect of in-store displays on log-of-demand.* 

*Model Description in B-Notation:*
$$ ln(y_t) = \beta_0 + \beta_1 S.WD.MIN + \beta_2 ln(A.PPU) +n_t $$ 
$$ (1-\phi_1 B - \phi_2 B^2 - \phi_3B^3)n_t = (1+\theta_1B + \theta_2 B^2)e_t $$



7. (5 pts) Use the model in Question 5 to prepare a 10 period ahead forecast and compare it (overly it) with the testing set log-of-sales data.  Comment on the usefulness of this model in terms of precision and confidence interval relative to the model without explanatory variables in Question 3.  

```{r, fig.height=4, fig.width=6}
#
# I am displaying the model is Question 6
# Forecasts from the model in Question 5 are somehat similar
#
fc <- forecast(m.SKX, h=10, xreg=xm.te[,c(4,6)])
plot(fc)
points(y.te, col="red", pch=19)
accuracy(fc,y.te)
```


8. (5 pts) After you complete a project, it is often useful to reflect on what would you different if you were to perform this project again.  This is no exception.  Comment on the training and testing fit statistics and discuss how do you think you could improve on the performance of the model in terms of (a) additional data, (b) different pre-processing of the existing data, and (c) different modeling choices.  Discuss your assessment of the potential for improvement (ex-ante priorities) for the different improvement options you suggest.

*If we were to perform this project again we may consider (among others) the following:*  

(a) *Additional Data:*  
      1. Advertising expenditures in mass media  
      2. Other promotional activities outside the store  
      
(b) *Different Preprocessing:*  
    1. Different aggregation of SKUs (i.e., crunchy, creamy, flavored)  
    2. A higher level of aggregation of advertising variables (some forms of advertising are seldom used)   
    
(c) *Modeling Choices:*    
    1. Possible effect of weather (temperature) on consumption patterns.
    2. Shifts in demand as a consequence of business (new or closing of a competitor-store) or economic environment (unemplyment, recession)
